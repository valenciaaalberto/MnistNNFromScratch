{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/mnist_train.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data.iloc[:,1:]\n",
    "y_train = data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W, b):\n",
    "    logit = W @ X.t + b\n",
    "    numerator = np.exp(logit)\n",
    "    denominator =  np.sum(np.exp(logit),axis=1)\n",
    "    ans = np.divide(numerator,denominator)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Defintion\n",
    "\n",
    "The loss function we select is the log cross entropy loss. It is defined as \n",
    "$$\n",
    "L(x,y;W,b) = -ln(p_{y}) = - \\sum_{i=0}^{K-1}ln(p_{i})\n",
    "$$\n",
    "where\n",
    "$$\n",
    "p_{y} = p(y = i|\\textbf{x}) = \\frac{e^{f_{i}}}{\\sum_{i=0}^{K-1} e^{f_{k}}}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "f_{i} = w_{j}\\textbf{x} + b_{b_{j}}\n",
    "$$\n",
    "To make the computation a bit easier, we can using a one hot encoding to mask all values not equal to the class in question to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n,K,Y):\n",
    "    Y_onehot = np.zeros((n,K))\n",
    "    for i in range(K):\n",
    "        for j in range(n):\n",
    "            if i != Y[j]:\n",
    "                continue\n",
    "            Y_onehot[j][i] = 1\n",
    "    return Y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(X, Y, W, b):\n",
    "    K = W.shape[0]\n",
    "    n = X.shape[0]\n",
    "    Y_onehot = one_hot(n,K,Y)\n",
    "    P = softmax(X,W,b)\n",
    "\n",
    "    L = np.sum((Y_onehot * np.log(P)) * -1)\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Gradient\n",
    "The gradient is given by \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = (P - Y)^TX\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\textbf{b}} = (P - Y)^T \\textbf{b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_prime(X, Y, W, b):\n",
    "    K = W.shape\n",
    "    n = X.shape[0]\n",
    "    P = softmax(X,W,b)\n",
    "    Y_onehot = one_hot(n,K,Y)\n",
    "    dL_by_dW =  ((P - Y_onehot).T).dot(X)\n",
    "    one = np.ones((n,1))\n",
    "    dL_by_db =  ((P - Y_onehot).T).dot(one)\n",
    "    return dL_by_dW, dL_by_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Defintion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# We will keep track of training loss over iterations.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m iterations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m L_list \u001b[38;5;241m=\u001b[39m [\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[1;32m     11\u001b[0m     gradient_W, gradient_b \u001b[38;5;241m=\u001b[39m L_prime(x_train, y_train,W,b)\n",
      "Cell \u001b[0;32mIn[74], line 4\u001b[0m, in \u001b[0;36mL\u001b[0;34m(X, Y, W, b)\u001b[0m\n\u001b[1;32m      2\u001b[0m K \u001b[38;5;241m=\u001b[39m W\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m n \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m Y_onehot \u001b[38;5;241m=\u001b[39m \u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m P \u001b[38;5;241m=\u001b[39m softmax(X,W,b)\n\u001b[1;32m      7\u001b[0m L \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((Y_onehot \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(P)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[73], line 5\u001b[0m, in \u001b[0;36mone_hot\u001b[0;34m(n, K, Y)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m----> 5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         Y_onehot[j][i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/core/series.py:974\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    973\u001b[0m key_is_scalar \u001b[38;5;241m=\u001b[39m is_scalar(key)\n\u001b[0;32m--> 974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    975\u001b[0m     key \u001b[38;5;241m=\u001b[39m unpack_1tuple(key)\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_should_fallback_to_positional:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K, D = x_train.shape\n",
    "learning_rate = 0.0025\n",
    "n_iter = 100       # Number of iterations\n",
    "W = np.zeros((K,D))   # Weight matrix.\n",
    "b = np.zeros((K,1))   # Bias vector.\n",
    "\n",
    "# We will keep track of training loss over iterations.\n",
    "iterations = [0]\n",
    "L_list = [L(x_train, y_train, W, b)]\n",
    "for i in range(n_iter):\n",
    "    gradient_W, gradient_b = L_prime(x_train, y_train,W,b)\n",
    "    W_new = W - learning_rate * gradient_W\n",
    "    b_new = b - learning_rate * gradient_b\n",
    "    \n",
    "    iterations.append(i+1)\n",
    "\n",
    "    L_list.append(L(x_train, y_train, W_new, b_new))\n",
    "    norm = np.abs(W_new-W).sum() + np.abs(b_new-b).sum()  # L1-norm as jumping out criteria.\n",
    "    if norm < 0.0005:\n",
    "        print(\"Gradient descent has converged after \" + str(i) + \" iterations.\")\n",
    "        break\n",
    "    W = W_new\n",
    "    b = b_new\n",
    "    \n",
    "print ('W matrix: \\n' + str(W))\n",
    "print ('b vector: \\n' + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Training curve')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('L(W,b)')\n",
    "plt.semilogy(iterations, np.array(L_list).reshape(-1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
