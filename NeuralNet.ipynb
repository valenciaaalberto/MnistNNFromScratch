{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mnist_train.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data.iloc[:,1:]\n",
    "y_train = data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W, b):\n",
    "    logit = W @ X.t + b\n",
    "    numerator = np.exp(logit)\n",
    "    denominator =  np.sum(np.exp(logit),axis=1)\n",
    "    ans = np.divide(numerator,denominator)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Defintion\n",
    "\n",
    "The loss function we select is the log cross entropy loss. It is defined as \n",
    "$$\n",
    "L(x,y;W,b) = -ln(p_{y}) = - \\sum_{i=0}^{K-1}ln(p_{i})\n",
    "$$\n",
    "where\n",
    "$$\n",
    "p_{y} = p(y = i|\\textbf{x}) = \\frac{e^{f_{i}}}{\\sum_{i=0}^{K-1} e^{f_{k}}}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "f_{i} = w_{j}\\textbf{x} + b_{b_{j}}\n",
    "$$\n",
    "To make the computation a bit easier, we can using a one hot encoding to mask all values not equal to the class in question to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n,K,Y):\n",
    "    Y_onehot = np.zeros((n,K))\n",
    "    for i in range(K):\n",
    "        for j in range(n):\n",
    "            if i != Y[j]:\n",
    "                continue\n",
    "            Y_onehot[j][i] = 1\n",
    "    return Y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(X, Y, W, b):\n",
    "    K = W.shape[0]\n",
    "    n = X.shape[0]\n",
    "    Y_onehot = one_hot(n,K,Y)\n",
    "    P = softmax(X,W,b)\n",
    "\n",
    "    L = np.sum((Y_onehot * np.log(P)) * -1)\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Gradient\n",
    "The gradient is given by \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = (P - Y)^TX\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\textbf{b}} = (P - Y)^T \\textbf{b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_prime(X, Y, W, b):\n",
    "    K = W.shape\n",
    "    n = X.shape[0]\n",
    "    P = softmax(X,W,b)\n",
    "    Y_onehot = one_hot(n,K,Y)\n",
    "    dL_by_dW =  ((P - Y_onehot).T).dot(X)\n",
    "    one = np.ones((n,1))\n",
    "    dL_by_db =  ((P - Y_onehot).T).dot(one)\n",
    "    return dL_by_dW, dL_by_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Defintion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, D = x_train.shape\n",
    "learning_rate = 0.0025\n",
    "epochs = 1      # Number of iterations\n",
    "W = np.zeros((K,D))   # Weight matrix.\n",
    "b = np.zeros((K,1))   # Bias vector.\n",
    "\n",
    "# We will keep track of training loss over iterations.\n",
    "iterations = [0]\n",
    "L_list = [L(x_train, y_train, W, b)]\n",
    "# for epoch in range(epochs):\n",
    "for i in range(len(x_train)):\n",
    "    gradient_W, gradient_b = L_prime(x_train[i], y_train[i],W,b)\n",
    "    W_new = W - learning_rate * gradient_W\n",
    "    b_new = b - learning_rate * gradient_b\n",
    "\n",
    "    iterations.append(i+1)\n",
    "    print(\"hello\")\n",
    "    L_list.append(L(x_train[i], y_train[i], W_new, b_new))\n",
    "    norm = np.abs(W_new-W).sum() + np.abs(b_new-b).sum()  # L1-norm as jumping out criteria.\n",
    "    if norm < 0.0005:\n",
    "        print(\"Gradient descent has converged after \" + str(i) + \" iterations.\")\n",
    "        break\n",
    "    W = W_new\n",
    "    b = b_new\n",
    "    \n",
    "print ('W matrix: \\n' + str(W))\n",
    "print ('b vector: \\n' + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Training curve')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('L(W,b)')\n",
    "plt.semilogy(iterations, np.array(L_list).reshape(-1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
